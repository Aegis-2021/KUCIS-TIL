# Softmax Regression (소프트맥스 회귀)

수정날짜: 2021년 7월 16일 오후 11:44
작성날짜: 2021년 7월 10일 오후 9:00

# Softmax Regression (소프트맥스 회귀)

---

## One-Hot Encoding

범주가 여러 개인 경우 데이터를 다음과 같이 인코딩해서 범주에 데이터를 동등하게 부여하는 것을 원핫 인코딩이라고 한다.

강아지 = [1, 0, 0]

고양이 = [0, 1, 0]

냉장고 = [0, 0, 1]

## 소프트맥스 함수

정의는 다음과 같다.

![Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled.png](https://github.com/Aegis-2021/KUCIS-TIL/blob/lwamuhaji/TIL/%ED%97%88%EC%A4%80%EC%84%9C/images/Softmax%20Regression%20(%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80)%20431acf88ba8b456ca0af9401d5341ea1/Untitled.png?raw=true)

4개의 특성을 입력으로 받아서 입력된 개체가 3가지 품종 중 어디에 해당하는지 알아내고 싶다고 하자. 그런 경우 아래 그림과 같이 소프트맥스 함수를 이용하여 구현할 수 있다.

### 1. 입력 벡터

![Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%201.png](https://github.com/Aegis-2021/KUCIS-TIL/blob/lwamuhaji/TIL/%ED%97%88%EC%A4%80%EC%84%9C/images/Softmax%20Regression%20(%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%201.png?raw=true)

위의 `모델에는 입력으로 4차원 벡터`가 들어간다. 하지만 소프트맥스 함수에 입력되는 벡터는 분류하고자 하는 클래스의 개수가 되어야 한다. 즉, 여기서는 `3차원 벡터`가 되어야 한다.

![Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%202.png](https://github.com/Aegis-2021/KUCIS-TIL/blob/lwamuhaji/TIL/%ED%97%88%EC%A4%80%EC%84%9C/images/Softmax%20Regression%20(%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%202.png?raw=true)

따라서 위 그림과 같이 각 독립변수에 `가중치 연산`을 해서 3차원 벡터로 변환한다. 위에서 화살표는 4 x3 = 12개이며 곱해지는 가중치는 서로 다르고, 학습을 통해 오차를 최소화하는 가중치로 값이 변경된다.

### 2. 오차 계산 - 크로스 엔트로피 함수

![Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%203.png](https://github.com/Aegis-2021/KUCIS-TIL/blob/lwamuhaji/TIL/%ED%97%88%EC%A4%80%EC%84%9C/images/Softmax%20Regression%20(%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%203.png?raw=true)

크로스 엔트로피 함수는 위와 같다. $y_j$는 실제값 원-핫 벡터의 j번째 인덱스를 의미하며, $p_j$는 샘플 데이터가 j번째 클래스일 확률을 나타낸다.

![Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%204.png](https://github.com/Aegis-2021/KUCIS-TIL/blob/lwamuhaji/TIL/%ED%97%88%EC%A4%80%EC%84%9C/images/Softmax%20Regression%20(%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%204.png?raw=true)

위 그림의 경우 크로스 엔트로피 함수를 적용하면 비용은 $-log(0.71)$이 된다.

![Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%205.png](https://github.com/Aegis-2021/KUCIS-TIL/blob/lwamuhaji/TIL/%ED%97%88%EC%A4%80%EC%84%9C/images/Softmax%20Regression%20(%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%205.png?raw=true)

이 비용을 모든 데이터 n개에 대하여 계산하여 평균을 내면 위와 같은 형태가 된다.

직접 코딩해서 확인해보자

```python
import torch
import torch.nn.functional as F
torch.manual_seed(1)

z = torch.FloatTensor([1, 2, 3])
hypothesis = F.softmax(z, dim=0)
print(hypothesis)
hypothesis.sum()
```

```python
tensor([0.0900, 0.2447, 0.6652])
tensor(1.)
```

각 수는 $\frac{e}{e+e^2+e^3},\frac{e^2}{e+e^2+e^3},\frac{e^3}{e+e^2+e^3}$ 이다. 또한 총 합이 1인 것을 알 수 있다.

```python
z = torch.rand(3, 5, requires_grad=True)
print(F.softmax(z, dim=0))
print(F.softmax(z, dim=1))
```

```python
tensor([[0.3787, 0.3363, 0.3350, 0.2610, 0.4237],
        [0.3517, 0.3391, 0.3522, 0.5176, 0.3441],
        [0.2696, 0.3245, 0.3128, 0.2213, 0.2322]], grad_fn=<SoftmaxBackward>)
tensor([[0.1794, 0.2427, 0.1770, 0.1301, 0.2708],
        [0.1549, 0.2276, 0.1731, 0.2399, 0.2045],
        [0.1625, 0.2979, 0.2104, 0.1404, 0.1888]], grad_fn=<SoftmaxBackward>)
```

3x5 크기의 텐서를 선언하고 소프트맥스 함수에 넣으면 위와 같이 나온다. dim=0인 경우 세로로 더했을 때 1이되고, dim=1인 경우 가로로 더했을 때 1이 된다.

```python
#가설을 설정하자
hypothesis = F.softmax(z, dim=1)
print(hypothesis)
#랜덤으로 0~4의 값을 갖는 크기가 (3,)인 텐서를 선언
y = torch.randint(5, (3,)).long()
print(y)
#y를 원핫 벡터로 변환하자
y_one_hot = torch.zeros_like(hypothesis)
print(y_one_hot)
y_one_hot.scatter_(1, y.unsqueeze(1), 1)
print(y_one_hot)
```

```python
tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],
        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],
        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)
tensor([0, 2, 1])
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]])
tensor([[1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 0.]])
```

이제 비용함수를 계산해보자.

![Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%206.png](https://github.com/Aegis-2021/KUCIS-TIL/blob/lwamuhaji/TIL/%ED%97%88%EC%A4%80%EC%84%9C/images/Softmax%20Regression%20(%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%206.png?raw=true)

이 비용함수를 그대로 코드로 구현하면 아래와 같이 된다.

```python
cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()
```

그런데 위 코드는 더 로우 레벨로 다음과 같이 나타낼 수 있다.

```python
cost = (y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()
```

softmax 함수와 log 함수가 합성되어 있는걸 볼 수 있다. 그런데 파이토치에서는 이 두 함수를 합친 F.log_softmax라는 함수를 제공한다. 따라서 위의 코드는 아래 코드와 정확히 같다.

```python
cost = (y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()
```

그런데 F.nll_loss 함수를 이용하면 원핫벡터를 만들 필요도 없이 그냥 y값을 넣어도 된다.

```python
cost = F.nll_loss(F.log_softmax(z, dim=1), y)
```

여기서 또 F.nll_loss 함수와 F.log_softmax 함수의 합성함수가 존재한다. 그래서 최종적으로 다음과 같이 쓸 수 있다.

```python
cost = F.cross_entropy(z, y)
```

그래서 아래의 4가지 코드는 모두 같다.

![Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%207.png](Softmax%20Regression%20(%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%20431acf88ba8b456ca0af9401d5341ea1/Untitled%207.png)

즉 입력값 z와 정답 라벨 y에 대해 z를 소프트맥스 함수에 넣고 예측값을 구한 후, 정답 y와의 오차를 계산하는게 위의 함수이다.
