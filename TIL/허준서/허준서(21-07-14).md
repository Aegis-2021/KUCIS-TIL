# 무작정 해보기

수정날짜: 2021년 7월 14일 오후 9:09
작성날짜: 2021년 7월 14일 오후 4:07

일단 실습용 가상환경을 생성한다.

아나콘다 네비게이터를 통해 puretorch 환경에 pytorch 설치

torch2 환경에 puretorch 환경 복사 후 네비게이터로 주피터 노트북 설치, torchtext 설치

## 문제

torchtext 패키지를 설치했는데도 모듈이 없다고 오류가 계속 뜬다.

→ wikidocs에서 사용한 버전과 내가 사용하는 버전이 달라서 그렇다.

```python
#from torchtext import data, datasets 이렇게 import하면 오류가 난다.
from torchtext.legacy import data, datasets
```

## 실습

영화 리뷰가 긍정인지 부정인지 학습하는 예제이다.

### 0. 기술

순환 신경망(Recurrent Neural Network, RNN)을 이용해서 학습한다. 자연어 처리에 이용되는 대표적인 기술로, 스팸메일 분류에도 RNN이 적용될 것이다.

### 1. 기본세팅

```python
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchtext.legacy import data, datasets
import random

SEED = 5
random.seed(SEED)
torch.manual_seed(SEED)

BATCH_SIZE = 64
lr = 0.001
EPOCHS = 10

USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")
print("cpu와 cuda 중 다음 기기로 학습함:", DEVICE)
```

모듈을 임포트하고 여러 변수를 설정해준다. 학습 환경도 확인해준다. 내 경우 cpu이다.

### 2. 전처리

```python
TEXT = data.Field(sequential=True, batch_first=True, lower=True)
LABEL = data.Field(sequential=False, batch_first=True)
```

두 Field 객체를 선언한다. 필드는 앞으로 전처리를 어떻게 진행할 것인지 정의하는 도구이다. 아래와 같은 파라미터를 통해 전처리를 어떻게 진행할지 자세하게 설정할 수 있다.

- sequential : 시퀀스 데이터 여부. (True가 기본값)
- use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값)
- tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)
- lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)
- batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부. (False가 기본값)
- is_target : 레이블 데이터 여부. (False가 기본값)
- fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다

```python
순차데이터란?
데이터 집합 내의 객체들이 가진 순서가 의미있는 경우를 말한다.
예를 들어, "나 사과 먹는다" 같은 데이터의 경우 순서가 뒤바뀌어 "먹는다 사과 나"과 같은
형태가 되는 경우 의미가 달라진다.
```

LABEL의 경우 클래스를 나타내는 단순한 숫자일 뿐이므로 순차 데이터가 아니므로 sequential=False로 설정해준다. 또한 lower=True를 통해 모든 데이터를 소문자로 바꿔준다.

```python
trainset, testset = datasets.IMDB.splits(TEXT, LABEL)
```

이제 trainset과 testset에 IMDB 데이터를 다운받고 할당해준다. 즉 데이터를 다운받으면서 훈련데이터와 테스트데이터를 분할하고, TEXT와 LABEL이 알려주는 전처리 과정에 따라 저장한다.

```python
print('trainset의 구성 요소 출력 : ', trainset.fields)
print('testset의 구성 요소 출력 : ', testset.fields)
```

```python
trainset의 구성 요소 출력 :  {'text': <torchtext.legacy.data.field.Field object at 0x00000287D1B94F88>, 'label': <torchtext.legacy.data.field.Field object at 0x00000287D1B94A88>}
testset의 구성 요소 출력 :  {'text': <torchtext.legacy.data.field.Field object at 0x00000287D1B94F88>, 'label': <torchtext.legacy.data.field.Field object at 0x00000287D1B94A88>}
```

trainset과 testset의 fields 속성이 위와 같이 설정되어 있는 것을 볼 수 있다.

```python
TEXT.build_vocab(trainset, min_freq=5) # 단어 집합 생성
LABEL.build_vocab(trainset)

vocab_size = len(TEXT.vocab)
n_classes = 2

print('단어 집합의 크기 : {}'.format(vocab_size))
print('클래스의 개수 : {}'.format(n_classes))
print(TEXT.vocab.stoi)
```

```python
단어 집합의 크기 : 46159
클래스의 개수 : 2
defaultdict(<function _default_unk_index at 0x7fb279f3cc80>, {'<unk>': 0, '<pad>': 1, 'the': 2, 'a': 3, 'and': 4, 'of': 5
... 중략 ...
'zoe,': 46150, 'zombies"': 46151, 'zombies)': 46152, 'zombified': 46153, 'zone.<br': 46154, 'zoolander': 46155, 'zwick': 46156, '{the': 46157, 'émigré': 46158})
```

단어집합(중복이 제거된 단어들의 총 합)도 만들어준다. 이떄 `min_freq=5`은 trainset에서 5번 이상 중복해서 나온 단어만을 단어 집합에 넣겠다는 의미이다. 5번 미만으로 나온 단어는 `'<unk>'` 라는 토큰으로 대체된다.

```python
trainset, valset = trainset.split(split_ratio=0.8)
```

아까전에 데이터를 훈련데이터와 테스트데이터로 분류했다. 이번에는 훈련데이터를 다시 8:2로 분류해서 훈련데이터와 검증데이터를 설정한다.

```python
train_iter, val_iter, test_iter = data.BucketIterator.splits(
        (trainset, valset, testset), batch_size=BATCH_SIZE,
        shuffle=True, repeat=False)
```

BucketIterator를 이용해 데이터를 배치 단위로 묶는다. 이렇게 하면 train_iter, val_iter, test_iter 각각에 샘플과 라벨이 64개 단위로 저장된다.

```python
print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))
print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))
print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))
```

```python
훈련 데이터의 미니 배치의 개수 : 313
테스트 데이터의 미니 배치의 개수 : 391
검증 데이터의 미니 배치의 개수 : 79
```

배치의 개수를 출력하면 위와 같이 나온다. (313 + 391 + 79) * 64 = 45504가 된다.

```python
batch = next(iter(train_iter)) # 첫번째 미니배치
print(batch.text.shape)
batch = next(iter(train_iter)) # 두번째 미니배치
print(batch.text.shape)
```

```python
torch.Size([64, 968])
torch.Size([64, 873])
```

미니배치의 크기를 출력하면 위와 같이 나온다. 아까 필드를 정의할 때 fix_length 속성 따로 정해주지 않았으므로 미니배치들 간의 샘플들의 길이는 다를 수 있다.

### 3. RNN모델 구현하기

```python
class GRU(nn.Module):
    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):
        super(GRU, self).__init__()
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim

        self.embed = nn.Embedding(n_vocab, embed_dim)
        self.dropout = nn.Dropout(dropout_p)
        self.gru = nn.GRU(embed_dim, self.hidden_dim,
                          num_layers=self.n_layers,
                          batch_first=True)
        self.out = nn.Linear(self.hidden_dim, n_classes)

    def forward(self, x):
        x = self.embed(x)
        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화
        x, _ = self.gru(x, h_0)  # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)
        h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.
        self.dropout(h_t)
        logit = self.out(h_t)  # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)
        return logit

    def _init_state(self, batch_size=1):
        weight = next(self.parameters()).data
        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()
```

모델을 구현한다. 코드를 하나씩 살펴보자.

```python
self.embed = nn.Embedding(n_vocab, embed_dim)
```

임베딩에 필요한 코드이다. 임베딩은 단어에 정수가 부여되고, 그 정수가 임베딩 층을 통과해서 밀집 벡터(또는 임베딩 벡터)로 맵핑되는 과정을 말한다.

![%E1%84%86%E1%85%AE%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%92%E1%85%A2%E1%84%87%E1%85%A9%E1%84%80%E1%85%B5%202ec78613be774b46b6030bdd557dbac5/Untitled.png](images/%E1%84%86%E1%85%AE%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC%20%E1%84%92%E1%85%A2%E1%84%87%E1%85%A9%E1%84%80%E1%85%B5%202ec78613be774b46b6030bdd557dbac5/Untitled.png)

위의 그림은 `great`이라는 단어가 `1918`이라는 정수값을 받고 `임베딩 층을 통과`해서 `벡터에 맵핑`되는 과정을 보여준다. 이때 테이블은 `단어집합만큼의 행`을 가진다. 또한 위의 테이블의 경우 `임베딩 벡터의 차원이 4로 설정`되어있다. 이제 이 임베팅 벡터는 모델의 입력값으로 쓰일 것이다.

즉 위의 코드에서는 nn.Embedding을 사용해 임베딩 테이블을 만든다. 파라미터로 임베딩할 단어들의 개수(단어집합의 크기)와 임베딩할 벡터의 차원을 받는다.

```python
self.dropout = nn.Dropout(dropout_p)
```

`Dropout`은 학습 과정에서 신경망의 일부를 사용하지 않는 것을 말한다. 이 클래스에서는 기본 0.2로 설정되어있으므로, 학습 과정마다 랜덤으로 80%의 뉴런만을 사용한다. 이를 통해 신경망이 특정 뉴런에 의존적이 되거나 과적합되는 것을 방지한다.

```python
self.gru = nn.GRU(embed_dim, self.hidden_dim,
                          num_layers=self.n_layers,
                          batch_first=True)
```

GRU는 RNN의 변형이다. 위 코드를 통해 GRU 셀을 만들게 된다. 이 GRU 셀의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)가 된다.

```python
self.out = nn.Linear(self.hidden_dim, n_classes)
```

선형회귀 모델을 설정한다. 입력의 크기가 은닉 상태의 크기이고 출력이 클래스의 개수로 설정되어있으므로 이 모델을 거치면 텐서의 크기는 (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)로 바뀐다.

### 4. 모델과 옵티마이저 설정

```python
model = GRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
```

모델의 경우 레이어의 개수는 1개, 은닉 상태의 크기는 256, 단어의 개수는 vocab_size, 임베딩 벡터의 크기는 128, 클래스의 개수는 n_classes, 드롭아웃 비율은 0.5로 설정되었다.

### 5. 모델 평가함수 설정

```python
def evaluate(model, val_iter):
    """evaluate model"""
    model.eval()
    corrects, total_loss = 0, 0
    for batch in val_iter:
        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)
        y.data.sub_(1) # 레이블 값을 0과 1로 변환
        logit = model(x)
        loss = F.cross_entropy(logit, y, reduction='sum')
        total_loss += loss.item()
        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()
    size = len(val_iter.dataset)
    avg_loss = total_loss / size
    avg_accuracy = 100.0 * corrects / size
    return avg_loss, avg_accuracy
```

### 6. 모델 훈련

```python
best_val_loss = None
for e in range(1, EPOCHS+1):
    train(model, optimizer, train_iter)
    val_loss, val_accuracy = evaluate(model, val_iter)

    print("[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f" % (e, val_loss, val_accuracy))

    # 검증 오차가 가장 적은 최적의 모델을 저장
    if not best_val_loss or val_loss < best_val_loss:
        if not os.path.isdir("snapshot"):
            os.makedirs("snapshot")
        torch.save(model.state_dict(), './snapshot/txtclassification.pt')
        best_val_loss = val_loss
```

```python
[Epoch: 1] val loss :  0.69 | val accuracy : 50.30
[Epoch: 2] val loss :  0.69 | val accuracy : 53.54
[Epoch: 3] val loss :  0.69 | val accuracy : 52.96
[Epoch: 4] val loss :  0.52 | val accuracy : 76.34
[Epoch: 5] val loss :  0.36 | val accuracy : 84.32
[Epoch: 6] val loss :  0.34 | val accuracy : 86.50
[Epoch: 7] val loss :  0.37 | val accuracy : 86.82
[Epoch: 8] val loss :  0.43 | val accuracy : 86.82
[Epoch: 9] val loss :  0.47 | val accuracy : 86.66
[Epoch: 10] val loss :  0.50 | val accuracy : 86.10
```

학습이 3시간이 넘게 걸렸다...

### 7. 평가

```python
model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))
test_loss, test_acc = evaluate(model, test_iter)
print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))
```

```python
테스트 오차:  0.30 | 테스트 정확도: 87.17
```

위에서 저장한 최적의 모델을 로드하고 테스트데이터를 통해 학습이 잘 되었는지 평가한다.
