# Aritificial Neural Network (인공 신경망)

Tech Stack: Pytorch
수정날짜: 2021년 7월 21일 오후 9:38
작성날짜: 2021년 7월 17일 오후 9:19

> 목차

# 1. Perceptron (퍼셉트론)

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled.png)

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%201.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%201.png)

입력값이 일정 수준을 넘기면 1, 그렇지 않으면 0을 출력값으로 내보내는 인공 신경망이다.

### a. 단층 퍼셉트론(Single-Layer Perceptron)

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%202.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%202.png)

단층 퍼셉트론에는 입력 단계와 출력 단계만이 존재한다. 이 단계들을 층이라고 부르는데, 따라서 단층 퍼셉트론은 입력층과 출력층만을 가진다.

단층 퍼셉트론을 이용하면 AND, OR, NAND 게이트를 구현할 수 있다.

```python
def AND_gate(x1, x2):
    w1=0.5
    w2=0.5
    b=-0.7
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%203.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%203.png)

```python
def NAND_gate(x1, x2):
    w1=-0.5
    w2=-0.5
    b=0.7
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%204.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%204.png)

```python
def OR_gate(x1, x2):
    w1=0.6
    w2=0.6
    b=-0.5
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%205.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%205.png)

위와 같이 각 게이트들은 점들이 직선 하나를 기준으로 나눠지게 된다. 하지만 XOR 같은 경우 직선 하나로만 점들을 나눌 수 없다. (단층 퍼셉트론은 선형 영역에 대해서만 분리가 가능하다.)

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%206.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%206.png)

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%207.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%207.png)

XOR 게이트는 위처럼 비선형 그래프를 통해 점들이 나눠져야한다.

### b. 다층 퍼셉트론(MultiLayer Perceptron, MLP)

위의 XOR 게이트는 NAND, OR, AND 게이트를 조합해서 구현할 수 있는데 이렇게 중간에 층이 하나 더 추가된 형태를 다층 퍼셉트론이라고 부른다. 또한 중간에 추가된 층을 은닉층이라고 한다.

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%208.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%208.png)

### c. 심층 신경망(Deep Neural Network, DNN)

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%209.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%209.png)

위와 같이 은닉층이 2개 이상한 신경망은 심층 신경망이라고 한다.

# 2. 단층 퍼셉트론 구현하기

- 사전준비

```python
import torch
import torch.nn as nn
device = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
```

- 입력 출력 정의

```python
X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)
Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)
```

- 모델 설계

```python
linear = nn.Linear(2, 1, bias=True)
sigmoid = nn.Sigmoid()
model = nn.Sequential(linear, sigmoid).to(device)
```

활성화 함수로 시그모이드 함수를 사용하고 입력되는 특성이 2개, 출력되는 특성이 1개이므로 리니어 함수를 위와 같이 설정한다.

- 비용함수와 옵티마이저 정의

```python
criterion = torch.nn.BCELoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=1)
```

BCELoss는 이진 분류에서 사용되는 크로스 엔트로피 함수이다.

- 학습

```python
for step in range(10001): 
    optimizer.zero_grad()
    hypothesis = model(X)

    # 비용 함수
    cost = criterion(hypothesis, Y)
    cost.backward()
    optimizer.step()

    if step % 100 == 0: # 100번째 에포크마다 비용 출력
        print(step, cost.item())
```

```python
0 0.7273974418640137
100 0.6931476593017578
200 0.6931471824645996
... 중략 ...
10000 0.6931471824645996
```

비용이 일정수준 이하로 떨어지지 않는다. 이는 저번에 공부했었듯이 XOR 게이트는 단층 퍼셉트론으로는 구현할 수 없기 때문이다.

# 3. 순전파와 역전파

### 1) 순전파

순전파는 입력층에서 출력층 방향으로 향하면서 값을 계산하는 단계를 말한다. 아래 예제를 통해 확인해보자.

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2010.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2010.png)

위 신경망에서 z는 가중합, h는 z가 활성화 함수를 거친 후 나온 값을 의미한다. 이 신경망에서는 활성화 함수로 시그모이드 함수를 이용한다.

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2011.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2011.png)

가중치와 입력값이 위와 같이 주어졌을 때 z값은 아래와 같다.

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2012.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2012.png)

z값을 시그모이드 함수에 넣으면 아래와 같다.

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2013.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2013.png)

따라서 z값은 다시 아래와 같이 계산된다.

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2014.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2014.png)

최종적으로 $\sigma$값은 아래와 같다.

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2015.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2015.png)

이제 손실함수를 계산하자. 손실함수로는 평균 제곱 오차함수를 이용한다.

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2016.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2016.png)

### 2) 역전파

역전파는 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트한다.

참고: [https://wikidocs.net/60682](https://wikidocs.net/60682)

파이토치는 위의 과정을 자동으로 대신 수행해주는 기능을 가지고 있다.

# 4. 다층 퍼셉트론 구현하기

아까 봤었던 XOR 게이트를 다층 퍼셉트론으로 구현해보자.

- 사전준비

```python
import torch
import torch.nn as nn

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# for reproducibility
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
```

- 입력과 출력 설정

```python
X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)
Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)
```

- 모델 설정

```python
model = nn.Sequential(
          nn.Linear(2, 10, bias=True), # input_layer = 2, hidden_layer1 = 10
          nn.Sigmoid(),
          nn.Linear(10, 10, bias=True), # hidden_layer1 = 10, hidden_layer2 = 10
          nn.Sigmoid(),
          nn.Linear(10, 10, bias=True), # hidden_layer2 = 10, hidden_layer3 = 10
          nn.Sigmoid(),
          nn.Linear(10, 1, bias=True), # hidden_layer3 = 10, output_layer = 1
          nn.Sigmoid()
          ).to(device)
```

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2017.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2017.png)

모델을 그림으로 표현한 것이다. 맨 왼쪽에 입력층, 맨 오른쪽에 출력층이 있고, 중간에 은닉층이 3개가 있다. 합성함수를 도식화할 때 나타는 그림과 거의 똑같다고 볼 수 있다. (입력이 여러 개의 출력에 연결되므로 정확히는 함수가 아니다.)

- 옵티마이저와 비용함수 설정

```python
criterion = torch.nn.BCELoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1
```

- 학습

```python
for epoch in range(10001):
    optimizer.zero_grad()
    # forward 연산
    hypothesis = model(X)

    # 비용 함수
    cost = criterion(hypothesis, Y)
    cost.backward()
    optimizer.step()

    # 100의 배수에 해당되는 에포크마다 비용을 출력
    if epoch % 100 == 0:
        print(epoch, cost.item())
```

- 평가

```python
with torch.no_grad():
    hypothesis = model(X)
    predicted = (hypothesis > 0.5).float()
    accuracy = (predicted == Y).float().mean()
    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())
    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())
    print('실제값(Y): ', Y.cpu().numpy())
    print('정확도(Accuracy): ', accuracy.item())
```

```python
모델의 출력값(Hypothesis):  [[1.1169249e-04]
 [9.9982882e-01]
 [9.9984229e-01]
 [1.8529959e-04]]
모델의 예측값(Predicted):  [[0.]
 [1.]
 [1.]
 [0.]]
실제값(Y):  [[0.]
 [1.]
 [1.]
 [0.]]
정확도(Accuracy):  1.0
```

모델이 잘 작동하는지 확인한다. 만약 예측값이 0.5가 넘는다면 0일 가능성보다는 1일 가능성이 크다는 것이기 때문에 1로 예측한 것으로 판단한다.

> *은닉층의 개수를 3개가 아니라 1개로 설정해도 모델이 잘 작동하는 걸 확인할 수 있었다.

# 5. 활성화 함수

### a. 비선형 활성화 함수

활성화 함수는 입력을 받아 수학적 변환을 수행하고 출력을 생성하는 함수이다. 앞서 살펴본 시그모이드 함수나 소프트맥스 함수는 대표적인 활성화 함수이다. 이때 이 활성화 함수는 비선형 함수여야 한다. 선형 함수는 여러번 합성해도 결국 비슷한 선형함수의 형태가 나오기 때문에 은닉층의 이점을 누리기 어렵다.

### b. 시그모이드 함수

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2018.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2018.png)

역전파 단계에서는 활성화 함수의 기울기가 곱해져서 가중치가 업데이트된다. 그런데 시그모이드 함수의 경우 위의 주황색 부분처럼 기울기가 0에 가까워지는 부분이 있다. 따라서 은닉층의 개수가 많은 경우 0에 가까운 값이 계속 곱해지면서 앞으로 갈수록 변화가 작아지게 된다. 이를 **기울기 소실**이라고 한다.

이런 이유로 시그모이드 함수를 은닉층에서 활성화 함수로 사용하는 것은 지양해야한다.

### c. 하이퍼볼릭탄젠트 함수

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2019.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2019.png)

시그모이드 함수와 비슷하게 생겼다. 하지만 0을 기준으로 두고 있고 시그모이드 함수보다는 기울기 소실 현상이 덜해서 시그모이드보다는 자주 쓰인다고 한다.

### d. 렐루 함수

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2020.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2020.png)

렐루 함수. 수식은 $f(x) = max(0,x)$ 로 간단하다. 기울기 소실 문제가 발생하지 않고 연산이 빠르기 때문에 가장 많이 쓰인다. 하지만 값이 0 이하인 경우 기울기가 0이 되기 때문에, 해당 뉴런이 죽어버리는 문제가 발생한다.

### e. 리키 렐루(Leaky ReLU)

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2021.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2021.png)

렐루의 뉴런이 죽는 문제를 보완하기 위해 나온 렐루 함수의 변형 버전이다.

### f. 소프트맥스 함수

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2022.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2022.png)

주로 다중 클래스 분류의 출력층에서 사용되는 함수이다.

# 6. 출력층의 활성화 함수

![Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2023.png](Aritificial%20Neural%20Network%20(%E1%84%8B%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A9%E1%86%BC%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC)%201616e13d515e448e9bd3e44fda8f6a7a/Untitled%2023.png)

은닉층의 활성화 함수로는 렐루와 리키렐루가 주로 쓰인다. 반면 출력층의 활성화 함수와 비용함수는 주로 위와 같이 쓰인다.