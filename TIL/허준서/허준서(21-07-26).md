# Overfitting, Gradient Vanishing and Exploding

Tech Stack: Pytorch
수정날짜: 2021년 7월 26일 오후 10:04
작성날짜: 2021년 7월 25일 오후 8:37

# Overfitting

모델이 학습에 사용되는 데이터에 과도하게 적응하는 현상이다. 학습 자체는 성공적으로 된 것처럼 보이지만 새로운 데이터가 주어지면 잘 예측하지 못하는 모습을 보인다. 해결책은 다음과 같다.

### 1. 데이터 양 늘리기

데이터의 양이 적으면 과적합이 일어나기 쉽다. 따라서 기존의 데이터를 변형하여 추가하는 방식으로 데이터를 늘리기도 한다. (Data Augmentation)

### 2. 모델 복잡도 줄이기

은닉층의 수나 매개변수의 수를 줄인다.

### 3. 가중치 규제

L1 규제: 가중치 w들의 절대값 합계를 비용 함수에 추가한다. 어떤 가중치가 모델에 영향을 주고 있는지를 정확하게 판단하고자 할 때 유용하다.

L2 규제: 모든 가중치 w들의 제곱합을 비용 함수에 추가한다. 일반적으로 L1 규제보다 L2 규제가 더 잘 작동하므로 L2 규제를 쓰는게 권장된다. 파이토치에서는 옵티마이저의 weight_decay 매개변수를 설정함으로써 L2 규제를 적용할 수 있다. 기본값은 0으로 설정되어있다.

### 4. Dropout

학습 과정에서 신경망의 일부를 사용하지 않는 것을 말한다. 드롭아웃 비율에 따라 얼만큼의 뉴런을 사용할지가 결정되고, 뉴런은 랜덤으로 선택된다.

![Overfitting,%20Gradient%20Vanishing%20and%20Exploding%20597256e6ff8342fb9add1e278e3ec510/Untitled.png](Overfitting,%20Gradient%20Vanishing%20and%20Exploding%20597256e6ff8342fb9add1e278e3ec510/Untitled.png)

# Gradient Vanishing & Exploding

깊은 인공 신경망을 학습할 때 역전파 과정에서 입력층으로 갈수록 기울기가 점점 작아지는 현상이다. 따라서 입력층에 가까워질수록 가중치들이 업데이트가 제대로 되지 않아 최적의 모델을 찾을 수 없게 되는 문제가 발생한다.

### 1. ReLU

가장 간단한 방법은 은닉층의 활성화 함수로 ReLU와 ReLU의 변형(Leaky ReLU)들을 사용하는 것이다.

### 2. Weight Initialization

모델의 구조가 같더라도 가중치의 초기값이 어떻게 설정되어 있느냐에 따라 학습의 결과가 달라질 수 있다. 즉 기울기 소실과 같은 문제는 가중치의 초기값을 적절히 설정해줌으로써 완화할 수 있다.

1. Xavier Initialization

    1) Uniform Distribution

    이전 층의 뉴런의 개수와 다음 층의 뉴런의 개수를 각각 $n_{in}, n_{out}$ 이라고 하면 Uniform Distribution을 따르는 경우 가중치는 다음과 같은 분포를 따르게 된다.

    $W \sim Uniform(-\sqrt{\frac{6}{n_{in}, n_{out}}}, +\sqrt{\frac{6}{n_{in}, n_{out}}})$

    2) Normal distribution

    정규분포로 초기화하는 경우 평균이 0, 표준편차는 $\sqrt{\frac{2}{n_{in}+n_{out}}}$ 이 되게 한다. 

    즉,  $W \sim N(0,\sqrt{\frac{2}{n_{in}+n_{out}}})$

    하지만 세이비어 초기화의 경우 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태의 활성화 함수에 쓰일 때 유효하다.

2. He Initialization

    ReLU 계열 함수를 쓸 때에는 He 초기화 방식을 쓴다. He 초기화도 세이비어 초기화와 비슷하게 균등 분포와 정규 분포로 나누어지는데 식은 다음과 같다.

    Uniform Distribution: $W \sim Uniform(-\sqrt{\frac{6}{n_{in}}}, +\sqrt{\frac{6}{n_{in}}})$

    Normal distribution: $W \sim N(0,\sqrt{\frac{2}{n_{in}}})$

    보통은 ReLU 함수와 He 초기화를 같이 쓴다.

### 3. Batch Normalization

배치 단위, 각 층에서 데이터가 활성화 함수를 통과하기 전에 이루어진다. 입력에 대해 아래와 같이 정규화한다.

![Overfitting,%20Gradient%20Vanishing%20and%20Exploding%20597256e6ff8342fb9add1e278e3ec510/Untitled%201.png](Overfitting,%20Gradient%20Vanishing%20and%20Exploding%20597256e6ff8342fb9add1e278e3ec510/Untitled%201.png)

![Overfitting,%20Gradient%20Vanishing%20and%20Exploding%20597256e6ff8342fb9add1e278e3ec510/Untitled%202.png](Overfitting,%20Gradient%20Vanishing%20and%20Exploding%20597256e6ff8342fb9add1e278e3ec510/Untitled%202.png)

다만 미니배치의 크기가 너무 작은 경우 분산이 극단적으로 작아지면서 학습에 악영향을 줄 수 있으므로, 미니배치의 크기가 충분히 큰 경우에 사용한다.

### 4. Layer Normalization

배치 정규화는 배치 사이즈에 의존적이기 때문에 RNN에 적용하기 어렵고, 배치 사이즈가 작은 경우 사용하기가 어려웠다. 하지만 층 정규화는 아래 그림과 같이 배치 사이즈에 의존적이지 않기 때문에 배치 정규화의 문제를 해결할 수 있다.

![Overfitting,%20Gradient%20Vanishing%20and%20Exploding%20597256e6ff8342fb9add1e278e3ec510/Untitled%203.png](Overfitting,%20Gradient%20Vanishing%20and%20Exploding%20597256e6ff8342fb9add1e278e3ec510/Untitled%203.png)