# 소프트맥스 회귀로 MNIST 데이터 학습

Tech Stack: Pytorch
수정날짜: 2021년 7월 19일 오후 10:26
작성날짜: 2021년 7월 15일 오후 10:46

# 0. MNIST 데이터란?

![%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%85%E1%85%A9%20MNIST%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20041ccaf3e8bc422b9a7d6e62b086c1c9/Untitled.png](%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%85%E1%85%A9%20MNIST%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20041ccaf3e8bc422b9a7d6e62b086c1c9/Untitled.png)

위와 같은 손글씨 데이터이다.

![%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%85%E1%85%A9%20MNIST%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20041ccaf3e8bc422b9a7d6e62b086c1c9/Untitled%201.png](%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%85%E1%85%A9%20MNIST%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20041ccaf3e8bc422b9a7d6e62b086c1c9/Untitled%201.png)

가로와 세로가 28픽셀인 이미지 데이터로, 총 784픽셀로 이루어져있다.

소프트맥스 회귀를 사용해 이미지가 어떤 숫자인지 예측하는 모델을 만들어보자.

MNIST 데이터는 보통 CNN으로 학습한다고 하는데 소프트맥스 회귀로도 가능하다.

# 1. 소프트맥스 회귀 학습하기

```python
import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.nn as nn
import matplotlib.pyplot as plt
import random

USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴
device = torch.device("cuda" if USE_CUDA else "cpu") # GPU 사용 가능하면 사용하고 아니면 CPU 사용
print("다음 기기로 학습합니다:", device)

random.seed(777)
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
    
training_epochs = 15
batch_size = 100
```

필요한 도구들을 임포트하고 하이퍼 파라미터를 설정해주자.

```python
mnist_train = dsets.MNIST(root='MNIST_data/',
                          train=True,
                          transform=transforms.ToTensor(),
                          download=True)

mnist_test = dsets.MNIST(root='MNIST_data/',
                         train=False,
                         transform=transforms.ToTensor(),
                         download=True)
```

토치비전을 이용해서 MNIST 데이터를 다운받는다. 각 파라미터의 의미는 다음과 같다.

- root: 데이터가 저장될 경로
- train: 훈련데이터인지, 테스트 데이터인지 여부
- transform: 데이터를 파이토치 텐서로 변환한다.
- download: 데이터가 없는 경우 다운받을 것인지 여부

```python
data_loader = DataLoader(dataset=mnist_train,
                                          batch_size=batch_size, # 배치 크기는 100
                                          shuffle=True,
                                          drop_last=True)
print(len(data_loader)) # 600
```

데이터를 다운받았으면 데이터로더를 만들어준다. 각 파라미터의 의미는 다음과 같다.

- dataset: 로드할 데이터
- batch_size: 배치크기
- shufle: 매 epoch마다 미니 배치를 셔플한 것인지
- drop_last: 마지막 배치를 버릴 것인지

마지막 배치를 버리는 이유는 데이터의 개수가 배치 크기로 나누어 떨어지지 않기 때문이다. 배치 크기가 일정하지 않은 경우 배치의 영향력이 달라지게 된다. (배치가 작을수록 과대평가된다.)

또한 데이터로더의 길이를 출력하면 600이 나온다. 배치크기가 100이었으므로 데이터의 총 개수는 60000개임을 확인할 수 있다.

```python
x, y = next(iter(data_loader))
print(x.shape)
print(y.shape)
```

```python
torch.Size([100, 1, 28, 28])
torch.Size([100])
```

데이터로더의 데이터와 라벨의 크기를 출력해보면 위와 같이 나온다.

```python
linear = nn.Linear(784, 10, bias=True).to(device)
```

모델을 설계한다. 이미지가 총 784픽셀로 이루어져 있었고, 데이터 클래스는 0부터 9까지 총 10가지이므로 input_dim은 784, output_dim은 10이다. bias는 디폴트 값이 True지만 명시해둠. to(device)의 경우 따로 설정하지 않으면 CPU로 설정된다.

```python
criterion = nn.CrossEntropyLoss().to(device) # 내부적으로 소프트맥스 함수를 포함하고 있음.
optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)
```

비용함수와 옵티마이저를 정의한다. 비용함수에는 nn.CrossEntropyLoss()가 쓰이며 내부에 소프트맥스 함수가 포함되어있다.

```python
for epoch in range(training_epochs): # 앞서 training_epochs의 값은 15로 지정함.
    avg_cost = 0
    total_batch = len(data_loader)

    for X, Y in data_loader:
        # 배치 크기가 100이므로 아래의 연산에서 X는 (100, 784)의 텐서가 된다.
        X = X.view(-1, 28 * 28).to(device)
        # 레이블은 원-핫 인코딩이 된 상태가 아니라 0 ~ 9의 정수.
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = linear(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))

print('Learning finished')
```

아까 보았듯 X의 크기는 (100, 1, 28, 28) 이었다. 크기를 view를 통해 X = X.view(-1, 28 * 28)와 같이 바꿔주면 크기는 (100, 784)가 된다. 아까 선형회귀 모델의 입력 텐서의 크기를 784로 설정했으므로 위와 같이 view를 통해 X를 바꿔줘야한다.

# 2. 모델 평가하기

```python
with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.
    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)

    prediction = linear(X_test)
    correct_prediction = torch.argmax(prediction, 1) == Y_test
    accuracy = correct_prediction.float().mean()
    print('Accuracy:', accuracy.item())

    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다
    r = random.randint(0, len(mnist_test) - 1)
    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)
    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)

    print('Label: ', Y_single_data.item())
    single_prediction = linear(X_single_data)
    print('Prediction: ', torch.argmax(single_prediction, 1).item())

    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')
    plt.show()
```

```python
prediction = linear(X_test)
print(prediction.shape)
```

```python
torch.Size([10000, 10])
```

아까 만든 선형 회귀 모델에 테스트 데이터를 넣어서 예측값을 얻는다. 선형 회귀 모델의 아웃 디멘션이 10이고 테스트 데이터의 개수가 1만개이므로 예측 텐서의 크기는 위와 같다.

![%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%85%E1%85%A9%20MNIST%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20041ccaf3e8bc422b9a7d6e62b086c1c9/Untitled%202.png](%E1%84%89%E1%85%A9%E1%84%91%E1%85%B3%E1%84%90%E1%85%B3%E1%84%86%E1%85%A2%E1%86%A8%E1%84%89%E1%85%B3%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%85%E1%85%A9%20MNIST%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%20%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20041ccaf3e8bc422b9a7d6e62b086c1c9/Untitled%202.png)

학습 과정을 보면 위와 같다. 하지만 평가 단계에서는 비용함수를 구할 필요가 없기 때문에 선형 회귀 모델에 데이터를 넣어 예측값을 얻는 것까지만 진행되는 것이다.

그리고 옵티마이저를 통해 변하는 파라미터는 선형 회귀 모델의 가중치와 편향이라는 것을 기억해야 한다. (소프트맥스 함수는 정적인 함수이다. 변하지 않는다!)

```python
correct_prediction = torch.argmax(prediction, 1) == Y_test
```

torch.argmax(prediction, 1)은 prediction 텐서의 두번째 차원에서 최댓값의 인덱스를 반환한다. 아까 두번째 차원의 크기가 10이었기 때문에 0~9가 반환된다. 그리고 이 인덱스가 Y_test와 같다면, 즉 정답이라면 true가 반환된다.

```python
accuracy = correct_prediction.float().mean()
```

이렇게 정확도를 구한다.

```python
r = random.randint(0, len(mnist_test) - 1)
X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)
Y_single_data = mnist_test.test_labels[r:r + 1].to(device)
```

랜덤 숫자 r을 통해 데이터와 라벨을 하나씩 골라오고, 

```python
print('Label: ', Y_single_data.item())
single_prediction = linear(X_single_data)
print('Prediction: ', torch.argmax(single_prediction, 1).item())
```

예측해서 보여준다.

[r]로 해도 될 것 같은데 위키독스에서는 [r:r+1]로 표현했다. 특별히 다른 이유가 있는진 모르겠는데 아마 범용성 떄문일 것 같다.