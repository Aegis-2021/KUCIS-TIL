# Linear Regression (선형 회귀)

Tech Stack: Pytorch
수정날짜: 2021년 7월 7일 오후 1:52
작성날짜: 2021년 6월 17일 오후 10:45

# 기본 세팅

---

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
```

사용할 모듈을 위와 같이 import 해준다.

 

```python
torch.manual_seed(1)
```

랜덤 시드를 1로 고정해서, 랜덤 값이 항상 같은 값이 나오도록 해준다. 이 경우 다른 컴퓨터에서 이 코드를 실행해도 같은 랜덤 값을 가지게 된다.

# 데이터 설정

---

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```

데이터를 위와 같이 선언해주고 가설은 $y = Wx + b$ 로 설정하자. 사실 위 데이터에서 목적함수는 y = 2x 라는 걸 쉽게 알 수 있긴 하다.

```python
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

가중치와 편향을 각각 0으로 초기화하고 requires_grad = True로 설정한다. 이는 두 변수가 학습을 통해 값이 변경되는 변수임을 의미한다.

```python
hypothesis = x_train * W + b
cost = torch.mean((hypothesis - y_train) ** 2)
```

가설을 설정하고 비용함수(평균 제곱 오차)를 설정해준다.

```python
optimizer = optim.SGD([W, b], lr=0.01) #확률적 경사 하강법 사용
cost.backward() #비용함수를 미분하여 기울기 계산
optimizer.step() #W와 b 업데이트
```

확률적 경사 하강법을 통해 W와 b를 최적화해준다.

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

optimizer = optim.SGD([W, b], lr=0.01)

nb_epochs = 1999
for epoch in range(nb_epochs + 1):

    hypothesis = x_train * W + b
    cost = torch.mean((hypothesis - y_train) ** 2)

    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W.item(), b.item(), cost.item()
        ))
```

![Linear%20Regression%20(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%202450233abe594db794446f1ef2d6442b/Untitled.png](Linear%20Regression%20(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%202450233abe594db794446f1ef2d6442b/Untitled.png)

전체코드와 실행결과는 위와 같다.

# 단순 선형 회귀 구현하기

---

아까는 가설과 비용함수를 직접 설정해서 선형 회귀 모델을 구현했지만, 이번에는 파이토치에서 제공하는 모듈을 이용해본다.

- 모듈 임포트 및 랜덤시드 고정

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
torch.manual_seed(1)
```

- 데이터 생성

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```

- 모듈 생성

```python
model = nn.Linear(1,1)
```

여기서 인자로 1,1을 넘겨주었으므로 단순 선형 회귀 모델이 되며, model에는 가중치와 편향이 저장되어있다. 이 값은 model.parameters()를 이용해 확인할 수 있다.

```python
print(list(model.parameters()))
```

[Parameter containing:
tensor([[0.5153]], requires_grad=True), Parameter containing:
tensor([-0.4414], requires_grad=True)]

현재는 랜덤한 값이 들어가있다.

- 옵티마이저 생성

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
```

learning rate가 0.01이고 확률적 경사하강법을 이용하는 옵티마이저를 설정한다. model.parameters()를 이용해 W와 b를 전달한다.

```python
nb_epochs = 2000
for epoch in range(nb_epochs+1):

    # H(x) 계산
    prediction = model(x_train)

    # cost 계산
    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수

    # cost로 H(x) 개선하는 부분
    # gradient를 0으로 초기화
    optimizer.zero_grad()
    # 비용 함수를 미분하여 gradient 계산
    cost.backward() # backward 연산
    # W와 b를 업데이트
    optimizer.step()

    if epoch % 100 == 0:
    # 100번마다 로그 출력
      print('Epoch {:4d}/{} Cost: {:.6f}'.format(
          epoch, nb_epochs, cost.item()
      ))
```

![Linear%20Regression%20(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%202450233abe594db794446f1ef2d6442b/Untitled%201.png](Linear%20Regression%20(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%202450233abe594db794446f1ef2d6442b/Untitled%201.png)

전체 코드와 실행결과는 위과 같다.

학습이 끝난 후 W와 b값을 확인해보면 아래와 같다.

![Linear%20Regression%20(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%202450233abe594db794446f1ef2d6442b/Untitled%202.png](Linear%20Regression%20(%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1)%202450233abe594db794446f1ef2d6442b/Untitled%202.png)